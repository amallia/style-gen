{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's generate some text\n",
    "\n",
    "No training here, only fun stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f0cb04c25c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai_old.text import *\n",
    "import html\n",
    "import spacy \n",
    "\n",
    "spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the itos/stoi have to match the model you're loading! Obvious but, you know. Remember.\n",
    "\n",
    "When we created the language model, we give it a path (in this case `data_inf/custom_lm/`). Then when we save the language model, it saves the data in `path/models/`. So we're pulling the models from `data_inf/custom_lm/models/`.\n",
    "\n",
    "If we want to load just the pre-trained data model in the same way, we need to load it up in the generating file and save it before it gets trained. These should be located in `data_inf/pretrained/`.\n",
    "\n",
    "Also I have set the batch size (`bs`) to 1 to avoid errors of processing the data. Since we're *not doing any training* this doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1830"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM_PATH = Path('data_inf/custom_lm/')\n",
    "# LM_PATH = Path('data_inf/pretrained/')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=1\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "# itos_name = 'itos_wt103.pkl'\n",
    "# model_name = 'pretrained'\n",
    "\n",
    "# itos_name = 'itos_imdb.pkl'\n",
    "# model_name = 'lm_last_ft'\n",
    "\n",
    "# itos_name = 'itos_alice_nop.pkl'\n",
    "# model_name = 'alice_nop_lm_30epochs'\n",
    "\n",
    "# itos_name = 'itos_alice_processed.pkl'\n",
    "# model_name = 'alice_processed_lm_30epochs'\n",
    "\n",
    "itos_name = 'itos_emily_nop.pkl'\n",
    "model_name = 'emily_nop_lm_30epochs'\n",
    "\n",
    "itos2 = pickle.load((LM_PATH/itos_name).open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})\n",
    "\n",
    "trn_lm = np.array([list([0,0])])\n",
    "val_lm = np.array([list([0,0])])\n",
    "\n",
    "vs=len(itos2)\n",
    "\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(LM_PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)\n",
    "\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.load(model_name)\n",
    "m = learner.model\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(m, s, l=20):\n",
    "    m[0].bs=1  # Set batch size to 1\n",
    "    m.eval()  # Turn off dropout\n",
    "    m.reset()  # Reset hidden state\n",
    "    m[0].bs=bs  # Put the batch size back to what it was\n",
    "\n",
    "    ss = s.lower().split()\n",
    "    si = [stoi2[w] for w in ss]\n",
    "    t = torch.autograd.Variable(torch.cuda.LongTensor(np.array([si])))\n",
    "    \n",
    "    res,*_ = m(t)\n",
    "\n",
    "    print(s, end=' ')\n",
    "    for i in range(l):\n",
    "#         n = res[-1].topk(5)[1]  # top word\n",
    "        n = torch.multinomial(res[-1].exp(), 2)  # drawing from probability distribution\n",
    "        n = n[1] if n.data[0]==0 else n[0]\n",
    "        print(itos2[int(n)], end=' ')\n",
    "        res,*_ = m(n.unsqueeze(0).unsqueeze(0))  # sometimes need an extra .unsqueeze(0)\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the  chair pleasure \n",
      "\n",
      "  length that passed bird n't . so smiling \n",
      "\n",
      "\n",
      "\n",
      " he dissent a ? in . , stand \n",
      "\n",
      " feels the ! , softly \n",
      "\n",
      "\n",
      "\n",
      "  . \n",
      "\n",
      " morning robins the first . room the till us water day of \n",
      " \n",
      "\n",
      "  angels , i . living , ix time the \n",
      "\n",
      "\n",
      "\n",
      " wondered \n",
      "\n",
      " t gain ; , , \n",
      "\n",
      "\n",
      "\n",
      " us mine troth morn warm . me the tenderer ; \n",
      "\n",
      " - \n",
      "\n",
      " are amid his me , perceived play . star \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  -- not \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " my have clear -- the \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " but i was on dimples ' go , smile ! ? it but , return centuries , -- that will xxviii , \n",
      " sometimes attitudes her \n",
      "\n",
      " \n",
      "\n",
      " know took . \n",
      "\n",
      " , \n",
      "\n",
      " prayer , ajar that \" but a me itself to t_up day tassels thou set , \n",
      " . . \n",
      "\n",
      "\n",
      " he ...\n"
     ]
    }
   ],
   "source": [
    "generate_text(m, \"the \", l=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason drawing from the distribution (using multinomial) produces garbage for the pre-trained model. It's fine for the fine-tuned model. Generally drawing from the distribution is better and more interesting; using just the top word is repetitive and boring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
